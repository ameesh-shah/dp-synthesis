{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "The following notebook contains code adapted from the Wrench tutorials (https://github.com/JieyuZ2/wrench.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import os\n",
    "\n",
    "import wrench.wrench as wrnch\n",
    "# import spacy\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "from wrench.wrench.dataset import load_dataset\n",
    "from wrench.wrench.logging import LoggingHandler\n",
    "from wrench.wrench.endmodel import MLPModel\n",
    "from wrench.wrench.labelmodel import MajorityVoting, FlyingSquid\n",
    "from typing import Any, List, Optional, Union, Callable\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Labeling Functions\n",
    "\n",
    "First, we need to load our data in to the Wrench-provided pipeline. A set of prior labeling features are provided in the dataset to use in lieu of user-provided labeling functions; in order to use them, set extract_feature to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the location from which to load the data.\n",
    "dataset_home = './data'\n",
    "data = 'youtube'\n",
    "#### Extract data features using pre-trained BERT model and cache it\n",
    "extract_fn = 'bert'\n",
    "model_name = 'bert-base-cased'\n",
    "\n",
    "#Note: you can set extract_features to True if you'd like to use pre-set Labeling Function outputs.\n",
    "train_data, valid_data, test_data = load_dataset(dataset_home, data, extract_feature=True, extract_fn=extract_fn,\n",
    "                                                 cache_name=extract_fn, model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining lfs\n",
    "def lf1(data):\n",
    "    if any(word in data for word in [\"<br\",\"amp\",\"Follow\",\"check\",\"channel\",\"plz\",\"PLEASE\",\"OUT\", \"fuck\", \"out\", \"SUBSCRIBE\" \"Check\",\"guys\", \"my\",\"My\", \"subscribe\", \"http\", \"https\", \"please\", \"href\", \"money\",\"$\", \"making\", \"per\"]):\n",
    "        return np.array(1)\n",
    "    else:\n",
    "        return np.array(-1)\n",
    "# def lf2(data):\n",
    "#     if \"my channel\" in data or \"leave some\" in data  or \"EXTRAORDINARY website\" in data or \"You can make\" in data or \"make money online\" in data:\n",
    "#         return np.array(1)\n",
    "#     else:\n",
    "#         return np.array(-1)\n",
    "# def lf3(data):\n",
    "#     if \"like this\" in data :\n",
    "#         return np.array(0)\n",
    "#     else:\n",
    "#         return np.array(-1)\n",
    "    \n",
    "def lf2(data):\n",
    "    if any(word in data for word in [\"!!!!\",\"PSY\",\"like\", \"song\",\"video\", \"music\" ]):\n",
    "        return np.array(0)\n",
    "    else:\n",
    "        return np.array(-1)\n",
    "\n",
    "# def lf5(data):\n",
    "#     if any(word in data for word in [\"PSY\",\"like\", \"song\" ]):\n",
    "#         return np.array(0)\n",
    "#     else:\n",
    "#         return np.array(-1)\n",
    "\n",
    "# def lf6(data):\n",
    "#     if any(word in data for word in [\"video\", \"music\"]):\n",
    "#         return np.array(0)\n",
    "#     else:\n",
    "#         return np.array(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you're going to manually generate and provide Labeling Functions, we're going to consider some ways of doing so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate procedural labeling functions, using the LF generation defined by WRENCH.\n",
    "from wrench.wrench.synthetic import ConditionalIndependentGenerator, NGramLFGenerator\n",
    "\n",
    "#### Generate procedural labeling functions\n",
    "# generator = NGramLFGenerator(dataset=train_data, min_acc_gain=0.1, min_support=0.01, ngram_range=(1, 2))\n",
    "# applier = generator.generate(mode='correlated', n_lfs=10)\n",
    "# L_test = applier.apply(test_data)\n",
    "# L_train = applier.apply(train_data)\n",
    "# print(len(train_data.examples))\n",
    "# print(L_train.shape)\n",
    "\n",
    "from labelfunction import LabelFunctionSet, RandomLFGenerator\n",
    "# ## Use our API to include and apply custom labeling functions.\n",
    "# LF_genr = RandomLFGenerator(num_functions=10, output_size=2) # 2 is the output size for binary classes\n",
    "# # As a placeholder, we're just going to generate 'random' labeling functions.\n",
    "# # If we want to use our own, we can just replace 'random_lfs' with our list of LFs!\n",
    "# random_lfs = LF_genr.get_random_lfs()\n",
    "\n",
    "LFSet = LabelFunctionSet()\n",
    "LFSet.add_function(lf1)\n",
    "LFSet.add_function(lf2)\n",
    "\n",
    "\n",
    "L_test = LFSet.apply_labels(test_data)\n",
    "L_train = LFSet.apply_labels(train_data)\n",
    "\n",
    "#### Evaluate label model on real-world dataset with semi-synthetic labeling functions\n",
    "label_model = MajorityVoting()\n",
    "label_model.fit(dataset_train=L_train, dataset_valid=valid_data)\n",
    "target_value = label_model.test(test_data, metric_fn='acc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure you run a cell to generate the labeled data if you're providing manual functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate soft training label via a label model\n",
    "soft_label = label_model.predict_proba(train_data)\n",
    "soft_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train a MLP classifier with soft label\n",
    "device = torch.device('cpu')\n",
    "n_steps = 100000\n",
    "batch_size = 128\n",
    "test_batch_size = 1000 \n",
    "patience = 200\n",
    "evaluation_step = 50\n",
    "target='acc'\n",
    "\n",
    "model = MLPModel(n_steps=n_steps, batch_size=batch_size, test_batch_size=test_batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's actually train the model here.\n",
    "history = model.fit(dataset_train=train_data, dataset_valid=valid_data, y_train=soft_label, \n",
    "                    device=device, metric=target, patience=patience, evaluation_step=evaluation_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Evaluate the trained model\n",
    "metric_value = model.test(test_data, target)\n",
    "print(metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Evaluate the trained model\n",
    "metric_value = model.test(test_data, target)\n",
    "print(metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrench",
   "language": "python",
   "name": "wrench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
