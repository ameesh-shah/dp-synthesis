{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf97eac",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "The following notebook contains code adapted from the Wrench tutorials (https://github.com/JieyuZ2/wrench.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c5f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import wrench.wrench as wrnch\n",
    "import spacy\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "from wrench.wrench.dataset import load_dataset\n",
    "from wrench.wrench.logging import LoggingHandler\n",
    "from wrench.wrench.endmodel import MLPModel\n",
    "from wrench.wrench.labelmodel import MajorityVoting, FlyingSquid\n",
    "from typing import Any, List, Optional, Union, Callable\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef78a1",
   "metadata": {},
   "source": [
    "# Creating Labeling Functions\n",
    "\n",
    "First, we need to load our data in to the Wrench-provided pipeline. A set of prior labeling features are provided in the dataset to use in lieu of user-provided labeling functions; in order to use them, set extract_feature to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c11b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-17 15:15:04 - loading data from data/youtube/train.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8dc3531dd9f456eb70f93bd9a4eff02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1686 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-17 15:15:04 - loading data from data/youtube/valid.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7e743f70a6467a92b59381358aeca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-17 15:15:04 - loading data from data/youtube/test.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49313c28ac2b47a0b1d25f8e75121307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-17 15:15:04 - loading features from data/youtube/train_bert.pkl\n",
      "2022-03-17 15:15:04 - loading features from data/youtube/valid_bert.pkl\n",
      "2022-03-17 15:15:04 - loading features from data/youtube/test_bert.pkl\n"
     ]
    }
   ],
   "source": [
    "# Set up the location from which to load the data.\n",
    "dataset_home = './data'\n",
    "data = 'youtube'\n",
    "#### Extract data features using pre-trained BERT model and cache it\n",
    "extract_fn = 'bert'\n",
    "model_name = 'bert-base-cased'\n",
    "\n",
    "#Note: you can set extract_features to True if you'd like to use pre-set Labeling Function outputs.\n",
    "train_data, valid_data, test_data = load_dataset(dataset_home, data, extract_feature=True, extract_fn=extract_fn,\n",
    "                                                 cache_name=extract_fn, model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59315ffa",
   "metadata": {},
   "source": [
    "Now, if you're going to manually generate and provide Labeling Functions, we're going to consider some ways of doing so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "245b022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generate procedural labeling functions, using the LF generation defined by WRENCH.\n",
    "from wrench.wrench.synthetic import ConditionalIndependentGenerator, NGramLFGenerator\n",
    "\n",
    "#### Generate procedural labeling functions\n",
    "# generator = NGramLFGenerator(dataset=train_data, min_acc_gain=0.1, min_support=0.01, ngram_range=(1, 2))\n",
    "# applier = generator.generate(mode='correlated', n_lfs=10)\n",
    "# L_test = applier.apply(test_data)\n",
    "# L_train = applier.apply(train_data)\n",
    "# print(len(train_data.examples))\n",
    "# print(L_train.shape)\n",
    "\n",
    "from labelfunction import LabelFunctionSet, RandomLFGenerator\n",
    "## Use our API to include and apply custom labeling functions.\n",
    "LF_genr = RandomLFGenerator(num_functions=10, output_size=2) # 2 is the output size for binary classes\n",
    "# As a placeholder, we're just going to generate 'random' labeling functions.\n",
    "# If we want to use our own, we can just replace 'random_lfs' with our list of LFs!\n",
    "random_lfs = LF_genr.get_random_lfs()\n",
    "LFSet = LabelFunctionSet(initial_functions=random_lfs)\n",
    "L_test = LFSet.apply_labels(test_data)\n",
    "L_train = LFSet.apply_labels(train_data)\n",
    "\n",
    "#### Evaluate label model on real-world dataset with semi-synthetic labeling functions\n",
    "label_model = FlyingSquid()\n",
    "label_model.fit(dataset_train=L_train, dataset_valid=valid_data)\n",
    "target_value = label_model.test(test_data, metric_fn='auc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56d758",
   "metadata": {},
   "source": [
    "#### Make sure you run a cell to generate the labeled data if you're providing manual functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1befede",
   "metadata": {},
   "source": [
    "# Downstream Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2777d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5],\n",
       "       [0.5, 0.5],\n",
       "       [0.5, 0.5],\n",
       "       ...,\n",
       "       [0.5, 0.5],\n",
       "       [0.5, 0.5],\n",
       "       [0.5, 0.5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Generate soft training label via a label model\n",
    "soft_label = label_model.predict_proba(train_data)\n",
    "soft_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3757a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train a MLP classifier with soft label\n",
    "device = torch.device('cuda:0')\n",
    "n_steps = 100000\n",
    "batch_size = 128\n",
    "test_batch_size = 1000 \n",
    "patience = 200\n",
    "evaluation_step = 50\n",
    "target='acc'\n",
    "\n",
    "model = MLPModel(n_steps=n_steps, batch_size=batch_size, test_batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447a42c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd12c15f7094187b2c69ad2e615a2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[TRAIN] MLP Classifier:   0%|                                                                                 â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-17 15:17:15 - [INFO] early stop @ step 12700!\n"
     ]
    }
   ],
   "source": [
    "# Let's actually train the model here.\n",
    "history = model.fit(dataset_train=train_data, dataset_valid=valid_data, y_train=soft_label, \n",
    "                    device=device, metric=target, patience=patience, evaluation_step=evaluation_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb3b0f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.608\n"
     ]
    }
   ],
   "source": [
    "#### Evaluate the trained model\n",
    "metric_value = model.test(test_data, target)\n",
    "print(metric_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777beae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wrench",
   "language": "python",
   "name": "wrench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
